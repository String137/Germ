{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pL7631cghuZj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from keras.layers import Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGOv0yA8hy2f"
   },
   "outputs": [],
   "source": [
    "BOARD_ROWS = 7\n",
    "BOARD_COLS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfELOA6sh3PJ"
   },
   "outputs": [],
   "source": [
    "class seven:\n",
    "    def aTp(self,Action):\n",
    "        position = np.zeros(4)\n",
    "        Action = int(Action)\n",
    "        for i in range(4):\n",
    "            position[3-i] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "            position = [int(i) for i in position]\n",
    "        return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnE6669eiIdt"
   },
   "outputs": [],
   "source": [
    "class Germ:\n",
    "    def __init__(self, alpha = 0.03, gamma = 0.95, epsilon = 0.1):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.board[0, 0] = self.board[BOARD_COLS-1, BOARD_COLS-1] = -1\n",
    "        self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = 1\n",
    "        # 우리가 1, 선공일 때를 나타냄\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = 64\n",
    "        self.min_replay_memory_size = 1000 # 얼마가 적당할지 잘 모르겠음.\n",
    "        self.target_update_freq = 100\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "\n",
    "        self.replay_memory_size = 5000\n",
    "        self.replay_memory = deque(maxlen=self.replay_memory_size)\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def cantmove(self): # 더 이상 움직일 수 없을 때 남은 곳을 상대 말로 채운다.\n",
    "        self.isEnd = True\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    self.board[i, j] = -self.playerSymbol\n",
    "        return None\n",
    "  \n",
    "    def winner(self): # 맵이 다 찼다면 점수를 반환한다.\n",
    "        if sum(map(sum,list(map(lambda row: list(map(abs,row)),self.board)))) == BOARD_ROWS*BOARD_COLS:\n",
    "            self.isEnd = True\n",
    "            return sum(map(sum, self.board))\n",
    "        return None\n",
    "\n",
    "    def availableActions(self): # 가능한 행동들을 반환한다.\n",
    "        Actions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == self.playerSymbol:\n",
    "                    for ii in range(-2,3):\n",
    "                        for jj in range(-2,3):\n",
    "                            if ii == 0 and jj == 0:\n",
    "                                continue\n",
    "                            if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
    "                                continue\n",
    "                            if self.board[i + ii, j + jj] == 0:\n",
    "                                act = i\n",
    "                                act = act*BOARD_COLS + j\n",
    "                                act = act*BOARD_COLS + i + ii\n",
    "                                act = act*BOARD_COLS + j + jj\n",
    "                                Actions.append(act)\n",
    "        return Actions\n",
    "\n",
    "    def isAvailableAction(self, Action): # 가능한 행동인지?\n",
    "        position = np.zeros(4)\n",
    "        Action = int(Action)\n",
    "        for i in range(4):\n",
    "            position[3-i] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "        position = [int(i) for i in position]\n",
    "        return self.board[position[0]][position[1]]==self.playerSymbol and self.board[position[2]][position[3]]==0\n",
    "\n",
    "    def updateState(self, Action): # 현재 상태에서 특정 행동을 한 다음 상태로 업데이트 한다.\n",
    "        position = np.zeros(4)\n",
    "        for t in range(4):\n",
    "            position[3-t] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "            position = [int(t) for t in position]\n",
    "        #print(Action, position)\n",
    "        ii = position[2] - position[0]\n",
    "        jj = position[3] - position[1]\n",
    "        if max(abs(ii), abs(jj)) == 2:\n",
    "            self.board[position[0],position[1]] = 0\n",
    "\n",
    "        dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
    "        dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
    "        i = position[2]\n",
    "        j = position[3]\n",
    "        self.board[i, j] = self.playerSymbol\n",
    "        for ii, jj in zip(dx1, dy1):\n",
    "            if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
    "                continue\n",
    "            if self.board[i + ii, j + jj] == -self.playerSymbol:\n",
    "                self.board[i + ii, j + jj] = self.playerSymbol\n",
    "\n",
    "\n",
    "    def reset(self): # 리셋.\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.board[0, 0] = self.board[BOARD_ROWS-1, BOARD_COLS-1] = -1\n",
    "        self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = 1\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "        \n",
    "    def showBoard(self):\n",
    "    # p1: o  p2: x\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('------------------------------')  \n",
    "\n",
    "    def build_model(self): # DQN 모델을 생성한다.\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'valid', input_shape=(7, 7, 1), dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'valid', input_shape=(5, 5, 1), dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64 * BOARD_COLS * BOARD_COLS, dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(BOARD_COLS**4, dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Reshape((BOARD_ROWS**4,), dtype='float32'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.alpha))\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, current_state, action, reward, next_state, done): # 리플레이 메모리에 상황을 저장한다.\n",
    "        self.replay_memory.append((current_state, action, reward, next_state, done))\n",
    "\n",
    "    def get_q_values(self, x): # 현재 상태에서 할 행동들의 q_value를 반환, x는 board에 대응됨.\n",
    "        return self.model.predict(x.reshape(1,BOARD_ROWS, BOARD_COLS, 1))\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "        # 무작위 행동 반환\n",
    "            avac_size = len(self.availableActions())\n",
    "            if avac_size == 0:\n",
    "                return None\n",
    "            return self.availableActions()[random.randrange(avac_size)]\n",
    "        else:\n",
    "        # 모델로부터 행동 산출\n",
    "            state = np.float32(state*self.playerSymbol)\n",
    "            q_values = self.model.predict(state.reshape(1,BOARD_ROWS,BOARD_COLS,1))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def epsbyepi(self, episode):\n",
    "        return max(self.epsilon, 1 - 1/(1+np.exp(-episode/15000+6)))\n",
    "    \n",
    "    def nextstate(self,state,episode):\n",
    "        next_state = np.zeros((7,7))\n",
    "        next_state = -state\n",
    "        self.playerSymbol*=-1\n",
    "        Action = self.getAction(state,self.epsbyepi(episode))\n",
    "        self.playerSymbol*=-1\n",
    "        if Action is None:\n",
    "            return np.zeros((7,7))\n",
    "        \n",
    "        position = np.zeros(4)\n",
    "        for i in range(4):\n",
    "            position[3-i] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "            position = [int(i) for i in position]\n",
    "        #print(Action, position)\n",
    "        ii = position[2] - position[0]\n",
    "        jj = position[3] - position[1]\n",
    "        if max(abs(ii), abs(jj)) == 2:\n",
    "            next_state[position[0],position[1]] = 0\n",
    "\n",
    "        dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
    "        dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
    "        i = position[2]\n",
    "        j = position[3]\n",
    "        next_state[i, j] = self.playerSymbol\n",
    "        for ii, jj in zip(dx1, dy1):\n",
    "            if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
    "                continue\n",
    "            if next_state[i + ii, j + jj] == -self.playerSymbol:\n",
    "                next_state[i + ii, j + jj] = self.playerSymbol\n",
    "        return -next_state\n",
    "\n",
    "\n",
    "    def play(self, episode):\n",
    "        prev_state = np.zeros((BOARD_ROWS,BOARD_COLS))\n",
    "        self.reset()\n",
    "        while not self.isEnd:\n",
    "            #self.showBoard()\n",
    "            avac = self.availableActions()\n",
    "            dora = False\n",
    "            if not avac:\n",
    "                self.cantmove()\n",
    "                win = self.winner()\n",
    "                if win is None:\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    reward = win * self.playerSymbol\n",
    "                    dora = True\n",
    "            else:\n",
    "                action = self.getAction(self.board, self.epsbyepi(episode)) # getAction에 playerSymbol 고려됨\n",
    "                state = self.board * self.playerSymbol\n",
    "                if self.isAvailableAction(action):\n",
    "                    self.updateState(action)\n",
    "                    win = self.winner()\n",
    "                    if win is None:\n",
    "                        reward = 0\n",
    "                    else:\n",
    "                        reward = win * self.playerSymbol\n",
    "                    dora = True\n",
    "                else:\n",
    "                    self.isEnd = True\n",
    "                    reward = -10\n",
    "            if self.isEnd and dora:\n",
    "                self.update_replay_memory(prev_state[0], prev_state[1], -reward, prev_state[2], False)\n",
    "            nt = self.nextstate(self.board,episode)\n",
    "            prev_state = (state, action, nt)\n",
    "            self.update_replay_memory(state, action, reward, nt, self.isEnd)\n",
    "            #print(self.playerSymbol)\n",
    "            # switch to another player\n",
    "            self.playerSymbol = -self.playerSymbol\n",
    "            \"\"\"\n",
    "            print(\"\")\n",
    "            print(\"state:\")\n",
    "            print(state)\n",
    "            print(seven.aTp(seven,action), reward)\n",
    "            print(\"updated:\")\n",
    "            print(self.board)\n",
    "            print(\"next:\")\n",
    "            print(nt)\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory)<self.min_replay_memory_size: # 충분히 모이지 않으면 학습하지 않는다.\n",
    "              return\n",
    "\n",
    "        samples = random.sample(self.replay_memory, self.batch_size)\n",
    "        current_input = np.stack([sample[0] for sample in samples]) # current_state들의 array\n",
    "        current_q_values = self.model.predict(current_input.reshape(len(current_input),BOARD_ROWS, BOARD_COLS,1))\n",
    "        next_input = np.stack([sample[3] for sample in samples])\n",
    "        next_q_values = self.target_model.predict(next_input.reshape(len(next_input),BOARD_ROWS, BOARD_COLS,1))\n",
    "\n",
    "        for i, (current_state, action, reward, _, done) in enumerate(samples):\n",
    "            if done:\n",
    "                next_q_value = reward\n",
    "            else:\n",
    "                next_q_value = reward + self.gamma * np.max(next_q_values[i])\n",
    "            current_q_values[i, action] = next_q_value\n",
    "            # print(current_state)\n",
    "            # print(next_input[i])\n",
    "            # print('-------------------')\n",
    "        current_input = current_input.reshape((len(current_input),BOARD_ROWS,BOARD_COLS,1))\n",
    "        hist = self.model.fit(current_input, current_q_values, batch_size=self.batch_size, verbose=1, shuffle=True, callbacks=tf.keras.callbacks.TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=True,\n",
    "    update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "    embeddings_metadata=None))\n",
    "        loss = hist.history['loss'][0]\n",
    "        return loss\n",
    "\n",
    "    def increase_target_update_counter(self): # target_model에 model을 업데이트한다. 그걸 세는 함수.\n",
    "        self.target_update_counter += 1\n",
    "        if self.target_update_counter >= self.target_update_freq:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def save(self, model_filepath, target_model_filepath):\n",
    "        self.model.save(model_filepath)\n",
    "        self.target_model.save(target_model_filepath)\n",
    "\n",
    "    def load(self, model_filepath, target_model_filepath):\n",
    "        self.model = keras.models.load_model(model_filepath)\n",
    "        self.target_model = keras.models.load_model(target_model_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kseW_GLhGWeq",
    "outputId": "e2b028c4-8e75-4ed4-8f9d-4eb0815724bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in dora.model.trainable_weights:\n",
    "    print(K.eval(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 5, 5, 64)          640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3136)              1809472   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2401)              7531937   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2401)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 2401)              0         \n",
      "=================================================================\n",
      "Total params: 9,378,977\n",
      "Trainable params: 9,378,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "round 0\n",
      "round 1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "round 2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0228e-04\n",
      "round 3000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1197e-04\n",
      "round 4000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5785e-04\n",
      "round 5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0056e-04\n",
      "round 6000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "round 7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1163e-04\n",
      "round 8000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "round 9000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5743e-04\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep10k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep10k03/target/assets\n",
      "round 10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "round 11000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "round 12000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "round 13000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "round 14000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "round 15000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9482\n",
      "round 16000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 51.7890\n",
      "round 17000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.8718\n",
      "round 18000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 78.8862\n",
      "round 19000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 173.4140\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep20k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep20k03/target/assets\n",
      "round 20000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2999.7097\n",
      "round 21000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 71134.9766\n",
      "round 22000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6743.3584\n",
      "round 23000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4297.8062\n",
      "round 24000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1912.8040\n",
      "round 25000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1140.9557\n",
      "round 26000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1268.3601\n",
      "round 27000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1737.5724\n",
      "round 28000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2522.1250\n",
      "round 29000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4166.9043\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep30k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep30k03/target/assets\n",
      "round 30000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4093.7866\n",
      "round 31000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13259.4131\n",
      "round 32000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13441.1396\n",
      "round 33000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42422.3438\n",
      "round 34000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 67136.3750\n",
      "round 35000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 68709.2188\n",
      "round 36000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36904.9570\n",
      "round 37000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41075.0352\n",
      "round 38000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14547.3184\n",
      "round 39000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15634.5449\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep40k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep40k03/target/assets\n",
      "round 40000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8590.2109\n",
      "round 41000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 65174.4688\n",
      "round 42000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12236.0820\n",
      "round 43000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8812.2451\n",
      "round 44000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9529.8516\n",
      "round 45000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6965.0654\n",
      "round 46000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3962.7344\n",
      "round 47000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2039.7278\n",
      "round 48000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2573.5327\n",
      "round 49000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3313.4241\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep50k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep50k03/target/assets\n",
      "round 50000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2622.9258\n",
      "round 51000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2477.8901\n",
      "round 52000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13197.4355\n",
      "round 53000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26729.8457\n",
      "round 54000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 46206.7656\n",
      "round 55000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93583.5469\n",
      "round 56000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255640.4375\n",
      "round 57000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 697041.9375\n",
      "round 58000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 384692.6562\n",
      "round 59000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 161770.7500\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep60k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep60k03/target/assets\n",
      "round 60000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 107481.3438\n",
      "round 61000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15119.7070\n",
      "round 62000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6993.0083\n",
      "round 63000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12166.3936\n",
      "round 64000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12845.0176\n",
      "round 65000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23855.0898\n",
      "round 66000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36462.9258\n",
      "round 67000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 47534.8516\n",
      "round 68000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 129645.3906\n",
      "round 69000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 174976.9688\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep70k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep70k03/target/assets\n",
      "round 70000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 154689.0781\n",
      "round 71000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17730.6543\n",
      "round 72000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5491.3711\n",
      "round 73000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5100.8413\n",
      "round 74000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5992.6338\n",
      "round 75000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6648.6270\n",
      "round 76000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6650.7939\n",
      "round 77000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4682.7646\n",
      "round 78000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5145.6475\n",
      "round 79000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5548.7666\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep80k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep80k03/target/assets\n",
      "round 80000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9453.6797\n",
      "round 81000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2550.6802\n",
      "round 82000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2728.0225\n",
      "round 83000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2355.9736\n",
      "round 84000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3885.0781\n",
      "round 85000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6428.7451\n",
      "round 86000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6335.2778\n",
      "round 87000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6150.6675\n",
      "round 88000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4031.7478\n",
      "round 89000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3061.1123\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep90k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep90k03/target/assets\n",
      "round 90000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2306.0159\n",
      "round 91000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1953.9302\n",
      "round 92000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11757.8945\n",
      "round 93000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12888.4082\n",
      "round 94000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12226.7402\n",
      "round 95000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5297.1221\n",
      "round 96000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8146.2881\n",
      "round 97000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1305.8262\n",
      "round 98000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 834.5431\n",
      "round 99000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1060.6487\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep100k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep100k03/target/assets\n",
      "round 100000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 969.8017\n",
      "round 101000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1118.7148\n",
      "round 102000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1052.5840\n",
      "round 103000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 639.6150\n",
      "round 104000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1015.2830\n",
      "round 105000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 798.5872\n",
      "round 106000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 599.7482\n",
      "round 107000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 806.0536\n",
      "round 108000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 536.5466\n",
      "round 109000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 801.6514\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep110k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep110k03/target/assets\n",
      "round 110000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 782.4122\n",
      "round 111000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 379.8139\n",
      "round 112000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 619.6028\n",
      "round 113000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 411.8450\n",
      "round 114000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 357.4998\n",
      "round 115000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 403.7892\n",
      "round 116000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 737.1332\n",
      "round 117000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 895.8853\n",
      "round 118000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 861.5001\n",
      "round 119000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 518.8676\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep120k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep120k03/target/assets\n",
      "round 120000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 649.9452\n",
      "round 121000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2363.4272\n",
      "round 122000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3599.3618\n",
      "round 123000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1876.2788\n",
      "round 124000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1874.2886\n",
      "round 125000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1344.3966\n",
      "round 126000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 952.1644\n",
      "round 127000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 968.7542\n",
      "round 128000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 614.9646\n",
      "round 129000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21266.3047\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep130k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep130k03/target/assets\n",
      "round 130000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 88857.7188\n",
      "round 131000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17442.9668\n",
      "round 132000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2922.9438\n",
      "round 133000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22052.4375\n",
      "round 134000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 61174.9805\n",
      "round 135000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36483.5391\n",
      "round 136000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16693.0039\n",
      "round 137000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4245.4014\n",
      "round 138000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5695.6807\n",
      "round 139000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7920.3237\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep140k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep140k03/target/assets\n",
      "round 140000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7356.0889\n",
      "round 141000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1210.6040\n",
      "round 142000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6034.6992\n",
      "round 143000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 9878.6719\n",
      "round 144000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1429.9462\n",
      "round 145000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 348.9229\n",
      "round 146000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 620.0606\n",
      "round 147000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1062.8979\n",
      "round 148000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1063.8394\n",
      "round 149000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 972.5969\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep150k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep150k03/target/assets\n",
      "round 150000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 856.2759\n",
      "round 151000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3267.2786\n",
      "round 152000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22166.2539\n",
      "round 153000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12465.7588\n",
      "round 154000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 27337.1895\n",
      "round 155000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 41241.4609\n",
      "round 156000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 57274.2266\n",
      "round 157000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 50365.4648\n",
      "round 158000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21557.7148\n",
      "round 159000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6744.0576\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep160k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep160k03/target/assets\n",
      "round 160000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2443.3704\n",
      "round 161000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11782.7969\n",
      "round 162000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15907.8037\n",
      "round 163000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10974.2266\n",
      "round 164000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3433.5708\n",
      "round 165000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3282.6807\n",
      "round 166000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1208.6504\n",
      "round 167000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 693.4733\n",
      "round 168000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 312.4436\n",
      "round 169000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 387.4011\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep170k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep170k03/target/assets\n",
      "round 170000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 454.0516\n",
      "round 171000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2496.8066\n",
      "round 172000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 731.6449\n",
      "round 173000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 895.3795\n",
      "round 174000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169.0300\n",
      "round 175000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 90.0901\n",
      "round 176000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 93.0224\n",
      "round 177000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 94.0508\n",
      "round 178000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 108.3676\n",
      "round 179000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 148.0332\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep180k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep180k03/target/assets\n",
      "round 180000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 157.2881\n",
      "round 181000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10967.0459\n",
      "round 182000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 166.4016\n",
      "round 183000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 653.5323\n",
      "round 184000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1175.6129\n",
      "round 185000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2130.0315\n",
      "round 186000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1627.6167\n",
      "round 187000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1390.6652\n",
      "round 188000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2049.0134\n",
      "round 189000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2153.9883\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep190k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep190k03/target/assets\n",
      "round 190000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1650.3800\n",
      "round 191000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2465.5635\n",
      "round 192000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 564.5261\n",
      "round 193000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 878.6727\n",
      "round 194000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 679.4507\n",
      "round 195000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 659.6173\n",
      "round 196000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 682.5543\n",
      "round 197000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1239.2971\n",
      "round 198000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1807.4661\n",
      "round 199000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2780.8418\n",
      "INFO:tensorflow:Assets written to: ./model/large_ep200k03/model/assets\n",
      "INFO:tensorflow:Assets written to: ./model/ep200k03/target/assets\n"
     ]
    }
   ],
   "source": [
    "dora = Germ()\n",
    "#dora.build_model()\n",
    "man = 0\n",
    "for kep in range(20): #사실 만임\n",
    "    if kep>0 or man>0:\n",
    "        dora.load(\"./model/large_ep{}k03/model\".format(kep*10+man*100),\"./model/ep{}k03/target\".format(kep*10+man*100))\n",
    "    episode = man*100000 + kep * 10000 + 1\n",
    "    for i in range(10000):\n",
    "        dora.play(episode + i)\n",
    "        dora.increase_target_update_counter()\n",
    "        if i%1000==0:\n",
    "            print(\"round\",man*100000+kep*10000+i)\n",
    "            dora.train()\n",
    "    dora.save(\"./model/large_ep{}k03/model\".format(man*100+kep*10+10),\"./model/ep{}k03/target\".format(man*100+kep*10+10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abc = np.zeros((7,7))\n",
    "abc[0,0]=abc[6,6]=1\n",
    "abc[0,6]=abc[6,0]=-1\n",
    "print(abc)\n",
    "qv = dora.model.predict(abc.reshape(1,abc.shape[0],abc.shape[1],1))\n",
    "print(qv)\n",
    "a=dora.model.get_weights()\n",
    "print(a)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dora = Germ()\n",
    "dora.build_model()\n",
    "dora.load(\"./ep20k/model\",\"./ep20k/target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "bNLCvesHXCfm",
    "outputId": "1c62b42b-4b74-476d-d613-e2cd592755de"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCndvt5ZXZYA"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self,isHuman,playerSymbol):\n",
    "        self.isHuman=isHuman\n",
    "        self.model = self.build_model()\n",
    "        self.playerSymbol = playerSymbol\n",
    "    \n",
    "    def build_model(self): # DQN 모델을 생성한다.\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(7, 7, 1), dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(5, 5, 1), dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64 * BOARD_COLS * BOARD_COLS, dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Dense(BOARD_COLS**4, dtype='float32'))\n",
    "        model.add(LeakyReLU(0.3))\n",
    "        model.add(Reshape((BOARD_ROWS**4,), dtype='float32'))\n",
    "        return model\n",
    "\n",
    "    def availableActions(self,state): # 가능한 행동들을 반환한다.\n",
    "        Actions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if state[i, j] == self.playerSymbol:\n",
    "                    for ii in range(-2,3):\n",
    "                        for jj in range(-2,3):\n",
    "                            if ii == 0 and jj == 0:\n",
    "                                continue\n",
    "                            if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
    "                                continue\n",
    "                            if state[i + ii, j + jj] == 0:\n",
    "                                act = i\n",
    "                                act = act*BOARD_COLS + j\n",
    "                                act = act*BOARD_COLS + i + ii\n",
    "                                act = act*BOARD_COLS + j + jj\n",
    "                                Actions.append(act)\n",
    "        return Actions    \n",
    "    def isAvailableAction(self, state, Action): # 가능한 행동인지?\n",
    "        position = np.zeros(4)\n",
    "        Action = int(Action)\n",
    "        for i in range(4):\n",
    "            position[3-i] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "            position = [int(i) for i in position]\n",
    "        return state[position[0]][position[1]]==self.playerSymbol and state[position[2]][position[3]]==0\n",
    "\n",
    "    \n",
    "    def getAction(self,state):\n",
    "        if self.isHuman:\n",
    "            print(\"original row, col, target row, col\")\n",
    "            ro = int(input())\n",
    "            co = int(input())\n",
    "            rt = int(input())\n",
    "            ct = int(input())\n",
    "            if not ro>=0 and ro<7 and co>=0 and co<7 and rt>=0 and rt<7 and ct>=0 and ct<7:\n",
    "                return None\n",
    "            act = ro*7*7*7+co*7*7+rt*7+ct\n",
    "            if not self.isAvailableAction(state,act):\n",
    "                return None\n",
    "            return act\n",
    "        else:\n",
    "            q_val = self.model.predict(state.reshape(1,BOARD_ROWS, BOARD_COLS, 1))\n",
    "            avac = self.availableActions(state)\n",
    "            if avac is None:\n",
    "                return None\n",
    "            avq = np.zeros(len(avac))\n",
    "            for i,a in enumerate(avac):\n",
    "                avq[i]=a\n",
    "                p=seven.aTp(seven,a)\n",
    "                print(\"(\",p[0],\",\",p[1],\") --> (\",p[2],\",\",p[3],\")   :  \",q_val[0,a])\n",
    "            for i in range(7):\n",
    "                for j in range(7):\n",
    "                    for k in range(7):\n",
    "                        for l in range(7):\n",
    "                            print(\"(\",i,\",\",j,\") --> (\",k,\",\",l,\")   :  \",\"{0:6f}\".format(q_val[0,343*i+49*j+7*k+l]))\n",
    "            return avq[np.argmax(q_val[0,avac])]\n",
    "\n",
    "    def load(self, model_filepath):\n",
    "        self.model = keras.models.load_model(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contest:\n",
    "    def __init__(self,p1,p2):\n",
    "        self.board = np.zeros((7,7))\n",
    "        self.board[0,6]=self.board[6,0]=1\n",
    "        self.board[0,0]=self.board[6,6]=-1\n",
    "        self.p1=p1\n",
    "        self.p2=p2\n",
    "        self.isEnd = False\n",
    "        \n",
    "    \n",
    "        \n",
    "    def updateState(self, Action, p): # 현재 상태에서 특정 행동을 한 다음 상태로 업데이트 한다.\n",
    "        dora = Action\n",
    "        position = np.zeros(4)\n",
    "        for i in range(4):\n",
    "            position[3-i] = Action % BOARD_COLS\n",
    "            Action = Action // BOARD_COLS\n",
    "        position = [int(i) for i in position]\n",
    "        ii = position[2] - position[0]\n",
    "        jj = position[3] - position[1]\n",
    "#         print(\"haha\")\n",
    "#         print(position)\n",
    "#         print(ii,jj)\n",
    "#         self.showBoard()\n",
    "#         print(self.board[position[0:2]])\n",
    "        if max(abs(ii), abs(jj)) == 2:\n",
    "            self.board[position[0],position[1]] = 0\n",
    "      \n",
    "        dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
    "        dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
    "        i, j = position[2:4]\n",
    "#         print(\"yay\")\n",
    "#         print(i,j)\n",
    "#         self.showBoard()\n",
    "        self.board[i, j] = p.playerSymbol\n",
    "        for ii, jj in zip(dx1, dy1):\n",
    "            if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
    "                continue\n",
    "            if self.board[i + ii, j + jj] == -p.playerSymbol:\n",
    "                self.board[i + ii, j + jj] = p.playerSymbol\n",
    "        print(\"(\",dora//(7**3),\" \",(dora//(7**2))%7,\" \",(dora//7)%7,\" \",dora%7,\")\")\n",
    "                \n",
    "    def winner(self): # 맵이 다 찼다면 점수를 반환한다.\n",
    "        if sum(map(sum, map(abs, self.board))) == BOARD_ROWS*BOARD_COLS:\n",
    "            self.isEnd = True\n",
    "            return sum(map(sum, self.board))\n",
    "        return None\n",
    "\n",
    "    def showBoard(self):\n",
    "    # p1: o  p2: x\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('------------------------------')  \n",
    "        \n",
    "    def cantmove(self,p): # 더 이상 움직일 수 없을 때 남은 곳을 상대 말로 채운다.\n",
    "        self.isEnd = True\n",
    "        for i in range(BOARD_ROWS):\n",
    "          for j in range(BOARD_COLS):\n",
    "            if self.board[i, j] == 0:\n",
    "              self.board[i, j] = -p.playerSymbol\n",
    "        return None\n",
    "        \n",
    "    def start(self): # 게임을 시작한다.\n",
    "        self.showBoard()\n",
    "        while not self.isEnd:\n",
    "            action1 = self.p1.getAction(self.board)\n",
    "            if action1 is None:\n",
    "                self.cantmove(self.p1)\n",
    "                break\n",
    "            self.updateState(action1,self.p1)\n",
    "            self.showBoard()\n",
    "            win=self.winner()\n",
    "            if win is not None:\n",
    "                self.isEnd = True\n",
    "                print(\"player\", int((3-np.sign(win))/2), \"win\")\n",
    "                break\n",
    "            action2 = self.p2.getAction(self.board)\n",
    "            if action2 is None:\n",
    "                self.cantmove(self.p2)\n",
    "                break\n",
    "            self.updateState(action2,self.p2)\n",
    "            self.showBoard()\n",
    "            win=self.winner()\n",
    "            if win is not None:\n",
    "                self.isEnd = True\n",
    "                print(\"player\", int((3-np.sign(win))/2), \"win\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dora = Player(False,-1)\n",
    "dora.load(\"./model/ep500k04/model\")\n",
    "jiyoon = Player(True,1)\n",
    "randi = Player(False,1)\n",
    "kapo = contest(jiyoon,dora)\n",
    "kapo.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dora.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Charm_Cracker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
