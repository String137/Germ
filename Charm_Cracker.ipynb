{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Charm_Cracker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7631cghuZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "from keras.layers import Reshape"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOv0yA8hy2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOARD_ROWS = 7\n",
        "BOARD_COLS = 7"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfELOA6sh3PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 참고 https://www.secmem.org/blog/2020/02/08/snake-dqn/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnE6669eiIdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Germ:\n",
        "  def __init__(self, alpha = 0.002, gamma = 0.95, epsilon = 0.1):\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "    self.board[0, 0] = self.board[BOARD_COLS-1, BOARD_COLS-1] = -1\n",
        "    self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = 1\n",
        "    # 우리가 1, 선공일 때를 나타냄\n",
        "    self.isEnd = False\n",
        "    self.playerSymbol = 1\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.batch_size = 64\n",
        "    self.min_replay_memory_size = 1000 # 얼마가 적당할지 잘 모르겠음.\n",
        "    self.target_update_freq = 100\n",
        "\n",
        "    self.model = self.build_model()\n",
        "    self.target_model = self.build_model()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "    self.model.summary()\n",
        "\n",
        "    self.replay_memory_size = 5000\n",
        "    self.replay_memory = deque(maxlen=self.replay_memory_size)\n",
        "    self.target_update_counter = 0\n",
        "\n",
        "  def cantmove(self): # 더 이상 움직일 수 없을 때 남은 곳을 상대 말로 채운다.\n",
        "    self.isEnd = True\n",
        "    for i in range(BOARD_ROWS):\n",
        "      for j in range(BOARD_COLS):\n",
        "        if self.board[i, j] == 0:\n",
        "          self.board[i, j] = -self.playerSymbol\n",
        "    return None\n",
        "  \n",
        "  def winner(self): # 맵이 다 찼다면 점수를 반환한다.\n",
        "    if sum(map(sum, map(abs, self.board))) == BOARD_ROWS*BOARD_COLS:\n",
        "      self.isEnd = True\n",
        "      return sum(map(sum, self.board))\n",
        "    return None\n",
        "\n",
        "  def availableActions(self): # 가능한 행동들을 반환한다.\n",
        "    Actions = []\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if self.board[i, j] == self.playerSymbol:\n",
        "                for ii in range(-2,3):\n",
        "                    for jj in range(-2,3):\n",
        "                        if ii == 0 and jj == 0:\n",
        "                          continue\n",
        "                        if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "                          continue\n",
        "                        if self.board[i + ii, j + jj] == 0:\n",
        "                          act = i\n",
        "                          act = act*BOARD_COLS + j\n",
        "                          act = act*BOARD_COLS + i + ii\n",
        "                          act = act*BOARD_COLS + j + jj\n",
        "                          Actions.append(act)\n",
        "    return Actions\n",
        "\n",
        "  def isAvailableAction(self, Action): # 가능한 행동인지?\n",
        "    position = np.zeros(4)\n",
        "    Action = int(Action)\n",
        "    for i in range(4):\n",
        "      position[3-i] = Action % BOARD_COLS\n",
        "      Action = Action // BOARD_COLS\n",
        "    position = [int(i) for i in position]\n",
        "    return self.board[position[0]][position[1]]==self.playerSymbol and self.board[position[2]][position[3]]==0\n",
        "\n",
        "  def updateState(self, Action): # 현재 상태에서 특정 행동을 한 다음 상태로 업데이트 한다.\n",
        "      position = np.zeros(4)\n",
        "      for i in range(4):\n",
        "        position[3-i] = Action % BOARD_COLS\n",
        "        Action = Action // BOARD_COLS\n",
        "      position = [int(i) for i in position]\n",
        "      #print(Action, position)\n",
        "      ii = position[2] - position[0]\n",
        "      jj = position[3] - position[1]\n",
        "      if max(abs(ii), abs(jj)) == 2:\n",
        "          self.board[position[0:2]] = 0\n",
        "      \n",
        "      dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
        "      dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
        "      i, j = position[2:4]\n",
        "      self.board[i, j] = self.playerSymbol\n",
        "      for ii, jj in zip(dx1, dy1):\n",
        "          if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "              continue\n",
        "          if self.board[i + ii, j + jj] == -self.playerSymbol:\n",
        "              self.board[i + ii, j + jj] = self.playerSymbol\n",
        "          \n",
        "      # switch to another player\n",
        "      self.playerSymbol = -self.playerSymbol\n",
        "\n",
        "  def reset(self): # 리셋.\n",
        "      self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "      self.board[0, 0] = self.board[BOARD_ROWS-1, BOARD_COLS-1] = 1\n",
        "      self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = -1\n",
        "      self.boardHash = None\n",
        "      self.isEnd = False\n",
        "      self.playerSymbol = 1\n",
        "    \n",
        "  def test(self):\n",
        "    while not self.isEnd:\n",
        "      avac = availableActions()\n",
        "      if not avac:\n",
        "        self.cantmove()\n",
        "      else:\n",
        "        state = self.board\n",
        "        state = np.float32(state)\n",
        "        q_vals = self.p1.model.predict(state)[0,0,:]\n",
        "        opt_action = 0\n",
        "        for action in self.availableActions:\n",
        "          if opt_action == 0:\n",
        "            opt_action = action\n",
        "          elif q_vals[opt_action]<q_vals[action]:\n",
        "            opt_action = action\n",
        "        self.updateState(opt_action)\n",
        "      self.showBoard()\n",
        "      win = self.winner()\n",
        "      if win is not None:\n",
        "        if win > 0:\n",
        "          print(self.p1.name, \"wins!\")\n",
        "        else:\n",
        "          print(self.p2.name, \"wins!\")\n",
        "        self.reset()\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        avac = availableActions()\n",
        "        if not avac:\n",
        "          self.cantmove()\n",
        "        else:\n",
        "          state = self.board\n",
        "          state = np.float32(state)\n",
        "          q_vals = self.p2.model.predict(state)[-1]\n",
        "          opt_action == 0\n",
        "          for action in self.availableActions:\n",
        "            if opt_action == 0:\n",
        "              opt_action = action\n",
        "            elif q_vals[opt_action]<q_vals[action]:\n",
        "              opt_action = action\n",
        "          self.updateState(opt_action)\n",
        "        self.showBoard()\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          if win > 0:\n",
        "            print(self.p2.name, \"wins!\")\n",
        "          else:\n",
        "            print(self.p1.name, \"wins!\")\n",
        "          print()\n",
        "          self.reset()\n",
        "          break\n",
        "        \n",
        "  def showBoard(self):\n",
        "    # p1: o  p2: x\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'o'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('------------------------------')  \n",
        "\n",
        "  def build_model(self): # DQN 모델을 생성한다.\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(7, 7, 1), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(5, 5, 1), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(3, 3, 1), activation='relu'))\n",
        "    model.add(Dense(64 * BOARD_COLS * BOARD_COLS, activation='relu'))\n",
        "    model.add(Dense(BOARD_COLS**4, activation='relu'))\n",
        "    model.add(Reshape((BOARD_ROWS**4,)))\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.alpha))\n",
        "    return model\n",
        "\n",
        "  def update_replay_memory(self, current_state, action, reward, next_state, done): # 리플레이 메모리에 상황을 저장한다.\n",
        "    self.replay_memory.append((current_state, action, reward, next_state, done))\n",
        "\n",
        "  def get_q_values(self, x): # 현재 상태에서 할 행동들의 q_value를 반환, x는 board에 대응됨.\n",
        "    return self.model.predict(x.reshape(1,BOARD_ROWS, BOARD_COLS, 1))\n",
        "\n",
        "  def getAction(self, state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "       # 무작위 행동 반환\n",
        "      avac_size = len(self.availableActions())\n",
        "      return self.availableActions()[random.randrange(avac_size)]\n",
        "    else:\n",
        "       # 모델로부터 행동 산출\n",
        "      state = np.float32(state*self.playerSymbol)\n",
        "      q_values = self.model.predict(state.reshape(1,BOARD_ROWS,BOARD_COLS,1))\n",
        "      return np.argmax(q_values)\n",
        "\n",
        "  def epsbyepi(self, episode):\n",
        "    return max(self.epsilon, 1 - 1/(1+np.exp(-episode/2500+6)))\n",
        "\n",
        "  def play(self, episode):\n",
        "    prev_state = np.zeros((BOARD_ROWS,BOARD_COLS))\n",
        "    self.reset()\n",
        "    while not self.isEnd:\n",
        "      #self.showBoard()\n",
        "      avac = self.availableActions()\n",
        "      dora = False\n",
        "      if not avac:\n",
        "        self.cantmove()\n",
        "      else:\n",
        "        action = self.getAction(self.board, self.epsbyepi(episode))\n",
        "        state = self.board * self.playerSymbol\n",
        "        if self.isAvailableAction(action):\n",
        "          self.updateState(action)\n",
        "          win = self.winner()\n",
        "          if win is None:\n",
        "            reward = 0\n",
        "          else:\n",
        "            reward = win * self.playerSymbol\n",
        "            dora = True\n",
        "        else:\n",
        "          self.isEnd = True\n",
        "          reward = -100\n",
        "      if self.isEnd and dora:\n",
        "        self.update_replay_memory(prev_state[0], prev_state[1], -reward, prev_state[2], False)\n",
        "      prev_state = (state, action, self.board)\n",
        "      self.update_replay_memory(state, action, reward, self.board, self.isEnd)\n",
        "      #print(action)\n",
        "      #print(state)\n",
        "      #print(win, reward, self.isEnd, dora, self.playerSymbol)\n",
        "    \n",
        "\n",
        "  def train(self):\n",
        "    if len(self.replay_memory)<self.min_replay_memory_size: # 충분히 모이지 않으면 학습하지 않는다.\n",
        "      return\n",
        "    \n",
        "    samples = random.sample(self.replay_memory, self.batch_size)\n",
        "    current_input = np.stack([sample[0] for sample in samples]) # current_state들의 array\n",
        "    current_q_values = self.model.predict(current_input.reshape(len(current_input),BOARD_ROWS, BOARD_COLS,1))\n",
        "    next_input = np.stack([sample[3] for sample in samples])\n",
        "    next_q_values = self.target_model.predict(next_input.reshape(len(next_input),BOARD_ROWS, BOARD_COLS,1))\n",
        "    \n",
        "    for i, (current_state, action, reward, _, done) in enumerate(samples):\n",
        "      if done:\n",
        "        next_q_value = reward\n",
        "      else:\n",
        "        next_q_value = reward + self.gamma * np.max(next_q_values[i])\n",
        "      current_q_values[i, action] = next_q_value\n",
        "    current_input = current_input.reshape((len(current_input),BOARD_ROWS,BOARD_COLS,1))\n",
        "    hist = self.model.fit(current_input, current_q_values, batch_size=self.batch_size, verbose=1, shuffle=False)\n",
        "    loss = hist.history['loss'][0]\n",
        "    return loss\n",
        "\n",
        "  def increase_target_update_counter(self): # target_model에 model을 업데이트한다. 그걸 세는 함수.\n",
        "    self.target_update_counter += 1\n",
        "    if self.target_update_counter >= self.target_update_freq:\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "      self.target_update_counter = 0\n",
        "\n",
        "  def save(self, model_filepath, target_model_filepath):\n",
        "    self.model.save(model_filepath)\n",
        "    self.target_model.save(target_model_filepath)\n",
        "\n",
        "  def load(self, model_filepath, target_model_filepath):\n",
        "    self.model = keras.models.load_model(model_filepath)\n",
        "    self.target_model = keras.models.load_model(target_model_filepath)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUHe5aWxyHpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "38720275-9766-491e-8443-01da551b0c08"
      },
      "source": [
        "dora = Germ()\n",
        "dora.build_model()\n",
        "# dora.load(뭐시기)\n",
        "episode = 1\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 5, 5, 16)          160       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1, 1, 3136)        53312     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 1, 2401)        7531937   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 2401)              0         \n",
            "=================================================================\n",
            "Total params: 7,590,049\n",
            "Trainable params: 7,590,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdypu2ZyxQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f90fd86-c705-4c3a-a6f4-81627aa194d5"
      },
      "source": [
        "for i in range(10000):\n",
        "  dora.play(episode)\n",
        "  episode += 1\n",
        "  dora.increase_target_update_counter()\n",
        "  if i%100==0 and i>0:\n",
        "    print(\"round\",i)\n",
        "    dora.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 100\n",
            "round 200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9093e-07\n",
            "round 300\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9756e-07\n",
            "round 400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4952e-07\n",
            "round 500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2503e-07\n",
            "round 800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.4322e-07\n",
            "round 900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8020e-07\n",
            "round 1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6390e-07\n",
            "round 1100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.3499e-07\n",
            "round 1200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6064e-07\n",
            "round 1300\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8771e-07\n",
            "round 1400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 1500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3807e-07\n",
            "round 1600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5983e-07\n",
            "round 1700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 1800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5049e-07\n",
            "round 1900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1292e-07\n",
            "round 2000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0624e-07\n",
            "round 2100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 2200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9954e-07\n",
            "round 2300\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8194e-07\n",
            "round 2400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.7132e-07\n",
            "round 2500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 2600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.8110e-07\n",
            "round 2700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6689e-07\n",
            "round 2800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4781e-07\n",
            "round 2900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4308e-07\n",
            "round 3000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.5135e-07\n",
            "round 3100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4736e-07\n",
            "round 3200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4577e-07\n",
            "round 3300\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5139e-07\n",
            "round 3400\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1302\n",
            "round 3500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5529e-07\n",
            "round 3600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5463e-07\n",
            "round 3700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4493e-07\n",
            "round 3800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4978e-07\n",
            "round 3900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4035e-07\n",
            "round 4000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4636e-07\n",
            "round 4100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5404e-07\n",
            "round 4200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4660e-07\n",
            "round 4300\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.5735e-07\n",
            "round 4400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 4500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5310e-07\n",
            "round 4600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6029e-07\n",
            "round 4700\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0651\n",
            "round 4800\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0651\n",
            "round 4900\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0651\n",
            "round 5000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 5100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 5200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.7503e-07\n",
            "round 5300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4957e-07\n",
            "round 5400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9414e-07\n",
            "round 5500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 5600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3905\n",
            "round 5700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 5800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 5900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1952\n",
            "round 6000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4269e-07\n",
            "round 6100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1902e-07\n",
            "round 6200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 6300\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1302\n",
            "round 6400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8757e-07\n",
            "round 6500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 6600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 6700\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0383e-07\n",
            "round 6800\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1302\n",
            "round 6900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 7000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0651\n",
            "round 7100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2604\n",
            "round 7200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 7300\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 7400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 7500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1953\n",
            "round 7600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1952\n",
            "round 7700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0651\n",
            "round 7800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2603\n",
            "round 7900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4390e-07\n",
            "round 8000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2604\n",
            "round 8100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1952\n",
            "round 8200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1954\n",
            "round 8300\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 8400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 8500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3906\n",
            "round 8600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 8700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2603\n",
            "round 8800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2604\n",
            "round 8900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5857\n",
            "round 9000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1302\n",
            "round 9100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 9200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 9300\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3255\n",
            "round 9400\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1952\n",
            "round 9500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3254\n",
            "round 9600\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2604\n",
            "round 9700\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4558\n",
            "round 9800\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1953\n",
            "round 9900\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kseW_GLhGWeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e2b028c4-8e75-4ed4-8f9d-4eb0815724bd"
      },
      "source": [
        "dora.save(\"/model/model10000\",\"/model/target10000\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /model/model10000/assets\n",
            "INFO:tensorflow:Assets written to: /model/target10000/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNLCvesHXCfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "1c62b42b-4b74-476d-d613-e2cd592755de"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model10000')\n",
        "files.download('target10000')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-a52a6d3f519b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    files.download('target10000'\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCndvt5ZXZYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}