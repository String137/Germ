{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Charm_Cracker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7631cghuZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOv0yA8hy2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOARD_ROWS = 7\n",
        "BOARD_COLS = 7"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfELOA6sh3PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 참고 https://www.secmem.org/blog/2020/02/08/snake-dqn/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnE6669eiIdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Germ:\n",
        "  def __init__(self, alpha = 0.2, gamma = 0.95, epsilon = 0.1):\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "    self.board[0, 0] = self.board[BOARD_COLS-1, BOARD_COLS-1] = -1\n",
        "    self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = 1\n",
        "    # 우리가 1, 선공일 때를 나타냄\n",
        "    self.isEnd = False\n",
        "    self.playerSymbol = 1\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.batch_size = 64\n",
        "    self.min_replay_memory_size = 1000 # 얼마가 적당할지 잘 모르겠음.\n",
        "    self.target_update_freq = 100\n",
        "\n",
        "    self.model = self.build_model()\n",
        "    self.target_model = self.build_model()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "    self.model.summary()\n",
        "\n",
        "    self.replay_memory_size = 5000\n",
        "    self.replay_memory = deque(maxlen=self.replay_memory_size)\n",
        "    self.target_update_counter = 0\n",
        "\n",
        "  def cantmove(self): # 더 이상 움직일 수 없을 때 남은 곳을 상대 말로 채운다.\n",
        "    self.isEnd = True\n",
        "    for i in range(BOARD_ROWS):\n",
        "      for j in range(BOARD_COLS):\n",
        "        if self.board[i, j] == 0:\n",
        "          self.board[i, j] = -self.playerSymbol\n",
        "    return None\n",
        "  \n",
        "  def winner(self): # 맵이 다 찼다면 점수를 반환한다.\n",
        "    if sum(map(sum, map(abs, self.board))) == BOARD_ROWS*BOARD_COLS:\n",
        "      self.isEnd = True\n",
        "      return sum(map(sum, self.board))\n",
        "    return None\n",
        "\n",
        "  def availableActions(self): # 가능한 행동들을 반환한다.\n",
        "    Actions = []\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if self.board[i, j] == self.playerSymbol:\n",
        "                for ii in range(-2,3):\n",
        "                    for jj in range(-2,3):\n",
        "                        if ii == 0 and jj == 0:\n",
        "                          continue\n",
        "                        if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "                          continue\n",
        "                        if self.board[i + ii, j + jj] == 0:\n",
        "                          act = i\n",
        "                          act = act*BOARD_COLS + j\n",
        "                          act = act*BOARD_COLS + i + ii\n",
        "                          act = act*BOARD_COLS + j + jj\n",
        "                          Actions.append(act)\n",
        "    return Actions\n",
        "\n",
        "  def isAvailableAction(self, Action): # 가능한 행동인지?\n",
        "    position = np.zeros(4)\n",
        "    Action = int(Action)\n",
        "    for i in range(4):\n",
        "      position[3-i] = Action % BOARD_COLS\n",
        "      Action = Action // BOARD_COLS\n",
        "    position = [int(i) for i in position]\n",
        "    return self.board[position[0]][position[1]]==self.playerSymbol and self.board[position[2]][position[3]]==0\n",
        "\n",
        "  def updateState(self, Action): # 현재 상태에서 특정 행동을 한 다음 상태로 업데이트 한다.\n",
        "      position = np.zeros(4)\n",
        "      for i in range(4):\n",
        "        position[3-i] = Action % BOARD_COLS\n",
        "        Action = Action // BOARD_COLS\n",
        "      position = [int(i) for i in position]\n",
        "      #print(Action, position)\n",
        "      ii = position[2] - position[0]\n",
        "      jj = position[3] - position[1]\n",
        "      if max(abs(ii), abs(jj)) == 2:\n",
        "          self.board[position[0:2]] = 0\n",
        "      \n",
        "      dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
        "      dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
        "      i, j = position[2:4]\n",
        "      self.board[i, j] = self.playerSymbol\n",
        "      for ii, jj in zip(dx1, dy1):\n",
        "          if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "              continue\n",
        "          if self.board[i + ii, j + jj] == -self.playerSymbol:\n",
        "              self.board[i + ii, j + jj] = self.playerSymbol\n",
        "          \n",
        "      # switch to another player\n",
        "      self.playerSymbol = -self.playerSymbol\n",
        "\n",
        "  def reset(self): # 리셋.\n",
        "      self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "      self.board[0, 0] = self.board[BOARD_ROWS-1, BOARD_COLS-1] = 1\n",
        "      self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = -1\n",
        "      self.boardHash = None\n",
        "      self.isEnd = False\n",
        "      self.playerSymbol = 1\n",
        "    \n",
        "  def test(self):\n",
        "    while not self.isEnd:\n",
        "      avac = availableActions()\n",
        "      if not avac:\n",
        "        self.cantmove()\n",
        "      else:\n",
        "        state = self.board\n",
        "        state = np.float32(state)\n",
        "        q_vals = self.p1.model.predict(state)[0,0,:]\n",
        "        opt_action = 0\n",
        "        for action in self.availableActions:\n",
        "          if opt_action == 0:\n",
        "            opt_action = action\n",
        "          elif q_vals[opt_action]<q_vals[action]:\n",
        "            opt_action = action\n",
        "        self.updateState(opt_action)\n",
        "      self.showBoard()\n",
        "      win = self.winner()\n",
        "      if win is not None:\n",
        "        if win > 0:\n",
        "          print(self.p1.name, \"wins!\")\n",
        "        else:\n",
        "          print(self.p2.name, \"wins!\")\n",
        "        self.reset()\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        avac = availableActions()\n",
        "        if not avac:\n",
        "          self.cantmove()\n",
        "        else:\n",
        "          state = self.board\n",
        "          state = np.float32(state)\n",
        "          q_vals = self.p2.model.predict(state)[-1]\n",
        "          opt_action == 0\n",
        "          for action in self.availableActions:\n",
        "            if opt_action == 0:\n",
        "              opt_action = action\n",
        "            elif q_vals[opt_action]<q_vals[action]:\n",
        "              opt_action = action\n",
        "          self.updateState(opt_action)\n",
        "        self.showBoard()\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          if win > 0:\n",
        "            print(self.p2.name, \"wins!\")\n",
        "          else:\n",
        "            print(self.p1.name, \"wins!\")\n",
        "          print()\n",
        "          self.reset()\n",
        "          break\n",
        "        \n",
        "  def showBoard(self):\n",
        "    # p1: o  p2: x\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'o'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('------------------------------')  \n",
        "\n",
        "  def build_model(self): # DQN 모델을 생성한다.\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(7, 7, 1), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(5, 5, 1), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(3, 3, 1), activation='relu'))\n",
        "    model.add(Dense(64 * BOARD_COLS * BOARD_COLS, activation='relu'))\n",
        "    model.add(Dense(BOARD_COLS**4, activation='relu'))\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.alpha))\n",
        "    return model\n",
        "\n",
        "  def update_replay_memory(self, current_state, action, reward, next_state, done): # 리플레이 메모리에 상황을 저장한다.\n",
        "    self.replay_memory.append((current_state, action, reward, next_state, done))\n",
        "\n",
        "  def get_q_values(self, x): # 현재 상태에서 할 행동들의 q_value를 반환, x는 board에 대응됨.\n",
        "    return self.model.predict(x)[0,0,:]\n",
        "\n",
        "  def getAction(self, state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "       # 무작위 행동 반환\n",
        "      avac_size = len(self.availableActions())\n",
        "      return self.availableActions()[random.randrange(avac_size)]\n",
        "    else:\n",
        "       # 모델로부터 행동 산출\n",
        "      state = np.float32(state*self.playerSymbol)\n",
        "      q_values = self.model.predict(state.reshape(1,BOARD_ROWS,BOARD_COLS,1))\n",
        "      print(q_values.shape)\n",
        "      print(q_values)\n",
        "      return np.argmax(q_values)\n",
        "\n",
        "  def epsbyepi(self, episode):\n",
        "    return max(self.epsilon, 1 - 1/(1+np.exp(-episode/250+6)))\n",
        "\n",
        "  def play(self, episode):\n",
        "    prev_state = np.zeros((BOARD_ROWS,BOARD_COLS))\n",
        "    self.reset()\n",
        "    while not self.isEnd:\n",
        "      #self.showBoard()\n",
        "      avac = self.availableActions()\n",
        "      dora = False\n",
        "      if not avac:\n",
        "        self.cantmove()\n",
        "      else:\n",
        "        action = self.getAction(self.board, self.epsbyepi(episode))\n",
        "        state = self.board * self.playerSymbol\n",
        "        if self.isAvailableAction(action):\n",
        "          self.updateState(action)\n",
        "          win = self.winner()\n",
        "          if win is None:\n",
        "            reward = 0\n",
        "          else:\n",
        "            reward = win * self.playerSymbol\n",
        "            dora = True\n",
        "        else:\n",
        "          self.isEnd = True\n",
        "          reward = -100\n",
        "      if self.isEnd and dora:\n",
        "        self.update_replay_memory(prev_state[0], prev_state[1], -reward, prev_state[2], False)\n",
        "      prev_state = (state, action, self.board)\n",
        "      self.update_replay_memory(state, action, reward, self.board, self.isEnd)\n",
        "      #print(action)\n",
        "      #print(state)\n",
        "      #print(win, reward, self.isEnd, dora, self.playerSymbol)\n",
        "    \n",
        "\n",
        "  def train(self):\n",
        "    if len(self.replay_memory)<self.min_replay_memory_size: # 충분히 모이지 않으면 학습하지 않는다.\n",
        "      return\n",
        "    \n",
        "    samples = random.sample(self.replay_memory, self.batch_size)\n",
        "    current_input = np.stack([sample[0] for sample in samples]) # current_state들의 array\n",
        "    current_q_values = self.model.predict(current_input)[0,0,:]\n",
        "    next_input = np.stack([sample[3] for sample in samples])\n",
        "    next_q_values = self.target_model.predict(next_input)[0,0,:]\n",
        "\n",
        "    for i, (current_state, action, reward, _, done) in enumerate(samples):\n",
        "      if done:\n",
        "        next_q_value = reward\n",
        "      else:\n",
        "        next_q_value = reward + self.gamma * np.max(next_q_values[i])\n",
        "      current_q_values[i, action] = next_q_value\n",
        "    current_input.reshape((current_input[0],current_input[1],current_input[2],1))\n",
        "    hist = self.model.fit(current_input, current_q_values, batch_size=self.batch_size, verbose=0, shuffle=False)\n",
        "    loss = hist.history['loss'][0]\n",
        "    return loss\n",
        "\n",
        "  def increase_target_update_counter(self): # target_model에 model을 업데이트한다. 그걸 세는 함수.\n",
        "    self.target_update_counter += 1\n",
        "    if self.target_update_counter >= self.target_update_freq:\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "      self.target_update_counter = 0\n",
        "\n",
        "  def save(self, model_filepath, target_model_filepath):\n",
        "    self.model.save(model_filepath)\n",
        "    self.target_model.save(target_model_filepath)\n",
        "\n",
        "  def load(self, model_filepath, target_model_filepath):\n",
        "    self.model = keras.models.load_model(model_filepath)\n",
        "    self.target_model = keras.models.load_model(target_model_filepath)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUHe5aWxyHpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d5405b6c-479f-4490-bec9-5e3d3d2d6012"
      },
      "source": [
        "dora = Germ()\n",
        "dora.build_model()\n",
        "# dora.load(뭐시기)\n",
        "episode = 1\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 5, 5, 16)          160       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1, 1, 3136)        53312     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 1, 2401)        7531937   \n",
            "=================================================================\n",
            "Total params: 7,590,049\n",
            "Trainable params: 7,590,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdypu2ZyxQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40e1d0ef-af77-4dba-df79-d93a86394cb0"
      },
      "source": [
        "for i in range(10000):\n",
        "  dora.play(episode)\n",
        "  episode += 1\n",
        "  dora.increase_target_update_counter()\n",
        "  if i%10==0 and i>0:\n",
        "    print(\"round\",i)\n",
        "  if i%100==0 and i>0:\n",
        "    dora.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 10\n",
            "round 20\n",
            "round 30\n",
            "round 40\n",
            "round 50\n",
            "(1, 1, 1, 2401)\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]]]]\n",
            "round 60\n",
            "(1, 1, 1, 2401)\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]]]]\n",
            "round 70\n",
            "(1, 1, 1, 2401)\n",
            "[[[[0.00016327 0.         0.         ... 0.         0.00066853\n",
            "    0.00111759]]]]\n",
            "(1, 1, 1, 2401)\n",
            "[[[[0.00017465 0.         0.00126646 ... 0.         0.\n",
            "    0.00112776]]]]\n",
            "round 80\n",
            "round 90\n",
            "(1, 1, 1, 2401)\n",
            "[[[[0.00030261 0.         0.00053138 ... 0.         0.\n",
            "    0.0011269 ]]]]\n",
            "round 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-748faeee0355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"round\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-45e3d2e7f9e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# current_state들의 array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0mnext_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [32, 7, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kseW_GLhGWeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9277b7d-2f6f-4fad-d4de-756b63e5770a"
      },
      "source": [
        "episode"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNLCvesHXCfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "519741d6-6433-49ec-dec3-1b1fd2dac4cd"
      },
      "source": [
        "6*7*7*7+6*7+1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCndvt5ZXZYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}