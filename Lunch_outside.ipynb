{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lunch_outside.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXGbSWGlkw8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivfGVf9Zk6sE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOARD_ROWS = 7\n",
        "BOARD_COLS = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS1iow0p_72c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # 참고 https://www.secmem.org/blog/2020/02/08/snake-dqn/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cip6XuuTlAHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class State:\n",
        "  def __init__(self, p1, p2):\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "    self.board[0, 0] = self.board[BOARD_ROWS-1, BOARD_COLS-1] = 1\n",
        "    self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = -1\n",
        "    self.p1 = p1\n",
        "    self.p2 = p2\n",
        "    self.isEnd = False\n",
        "    self.boardHash = None\n",
        "    # init p1 plays first\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "  def cantmove(self): # 더 이상 움직일 수 없을 때 남은 곳을 상대 말로 채운다.\n",
        "    for i in range(BOARD_ROWS):\n",
        "      for j in range(BOARD_COLS):\n",
        "        if self.board[i, j] == 0:\n",
        "          self.board[i, j] = -self.playerSymbol\n",
        "    return None\n",
        "\n",
        "  def winner(self): # 맵이 다 찼다면 점수를 반환한다.\n",
        "    if sum(map(sum, map(abs, self.board))) == BOARD_ROWS*BOARD_COLS:\n",
        "      self.isEnd = True\n",
        "      return sum(map(sum, self.board))\n",
        "    self.isEnd = False\n",
        "    return None\n",
        "\n",
        "  def availableActions(self): # 가능한 행동들을 반환한다.\n",
        "    Actions = []\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if self.board[i, j] == self.playerSymbol:\n",
        "                for ii in range(-2,3):\n",
        "                    for jj in range(-2,3):\n",
        "                        if ii == 0 and jj == 0:\n",
        "                          continue\n",
        "                        if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "                          continue\n",
        "                        if self.board[i + ii, j + jj] == 0:\n",
        "                          act = i\n",
        "                          act = act*BOARD_COLS + j\n",
        "                          act = act*BOARD_COLS + i + ii\n",
        "                          act = act*BOARD_COLS + j + jj\n",
        "                          Actions.append(act)\n",
        "    return Actions\n",
        "\n",
        "\n",
        "  def isAvailabeAction(self, Action): # 가능한 행동인지?\n",
        "    position = []\n",
        "        for i in range(4):\n",
        "          position[3-i] = Action % BOARD_COLS\n",
        "          Action /= BOARD_COLS\n",
        "    return self.board[position[0],position[1]]==self.playerSymbol and self.board[position[2],position[3]]==0\n",
        "\n",
        "\n",
        "  def updateState(self, Action): # 현재 상태에서 특정 행동을 한 다음 상태로 업데이트 한다.\n",
        "      position = []\n",
        "      for i in range(4):\n",
        "        position[3-i] = Action % BOARD_COLS\n",
        "        Action /= BOARD_COLS\n",
        "      ii = position[2] - position[0]\n",
        "      jj = position[3] - position[1]\n",
        "      if max(abs(ii), abs(jj)) == 2:\n",
        "          self.board[position[0:2]] = 0\n",
        "      \n",
        "      dx1 = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
        "      dy1 = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
        "      i, j = position[2:4]\n",
        "      self.board[i, j] = self.playerSymbol\n",
        "      for ii, jj in zip(dx1, dy1):\n",
        "          if i + ii < 0 or i + ii >= BOARD_ROWS or j + jj < 0 or j + jj >= BOARD_COLS:\n",
        "              continue\n",
        "          if self.board[i + ii, j + jj] == -self.playerSymbol:\n",
        "              self.board[i + ii, j + jj] = self.playerSymbol\n",
        "          \n",
        "      # switch to another player\n",
        "      self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
        "\n",
        "  def giveReward(self): # 보상을 준다.\n",
        "      result = self.winner()\n",
        "      # backpropagate reward\n",
        "      self.p1.feedReward(result)\n",
        "      self.p2.feedReward(-result)\n",
        "\n",
        "  def reset(self): # 리셋.\n",
        "      self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "      self.board[0, 0] = self.board[BOARD_ROWS-1, BOARD_COLS-1] = 1\n",
        "      self.board[BOARD_ROWS-1, 0] = self.board[0, BOARD_COLS-1] = -1\n",
        "      self.boardHash = None\n",
        "      self.isEnd = False\n",
        "      self.playerSymbol = 1\n",
        "  def test(self):\n",
        "    while not self.isEnd:\n",
        "      avac = availableActions()\n",
        "      if not avac:\n",
        "        self.cantmove()\n",
        "      else:\n",
        "        state = self.board\n",
        "        state = np.float32(state)\n",
        "        q_vals = self.p1.model.predict(state)\n",
        "        opt_action = 0\n",
        "        for action in self.availableActions:\n",
        "          if opt_action == 0:\n",
        "            opt_action = action\n",
        "          elif q_vals[opt_action]<q_vals[action]:\n",
        "            opt_action = action\n",
        "        self.updateState(opt_action)\n",
        "      self.showBoard()\n",
        "      win = self.winner()\n",
        "      if win is not None:\n",
        "        if win > 0:\n",
        "          print(self.p1.name, \"wins!\")\n",
        "        else:\n",
        "          print(self.p2.name, \"wins!\")\n",
        "        self.reset()\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        avac = availableActions()\n",
        "        if not avac:\n",
        "          self.cantmove()\n",
        "        else:\n",
        "          state = self.board\n",
        "          state = np.float32(state)\n",
        "          q_vals = self.p2.model.predict(state)\n",
        "          opt_action == 0\n",
        "          for action in self.availableActions:\n",
        "            if opt_action == 0:\n",
        "              opt_action = action\n",
        "            elif q_vals[opt_action]<q_vals[action]:\n",
        "              opt_action = action\n",
        "          self.updateState(opt_action)\n",
        "        self.showBoard()\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          if win > 0:\n",
        "            print(self.p2.name, \"wins!\")\n",
        "          else:\n",
        "            print(self.p1.name, \"wins!\")\n",
        "          print()\n",
        "          self.reset()\n",
        "          break\n",
        "        \n",
        "  def showBoard(self):\n",
        "    # p1: o  p2: x\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'o'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('------------------------------')  \n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ksa-VSTmN-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Player:\n",
        "  def __init__(self, name, alpha=0.2, gamma=0.95, epsilon=0.1):\n",
        "    self.name = name\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.batch_size = 1000\n",
        "    self.min_replay_memory_size = 1000 # 얼마가 적당할지 잘 모르겠음.\n",
        "    self.target_update_freq = 100\n",
        "\n",
        "    self.model = self.build_model()\n",
        "    self.target_model = self.build_model()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "    self.model.summary()\n",
        "\n",
        "    self.replay_memory = deque(maxlen=replay_memory_size)\n",
        "    self.target_update_counter = 0\n",
        "    \n",
        "\n",
        "\n",
        "  def build_model(self): # DQN 모델을 생성한다.\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(7, 7), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(5, 5), activation='relu'))\n",
        "    model.add(Conv2D(16, (3, 3), padding = 'valid', input_shape=(3, 3), activation='relu'))\n",
        "    model.add(Dense(64 * BOARD_COLS * BOARD_COLS, activation='relu'))\n",
        "    model.add(Dense(BOARD_COLS**4, activation='relu'))\n",
        "    print(model.summary())\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def update_replay_memory(self, current_state, action, reward, next_state, done): # 리플레이 메모리에 상황을 저장한다.\n",
        "    self.replay_memory.append((current_state, action, reward, next_state, done))\n",
        "\n",
        "  def get_q_values(self, x): # 현재 상태에서 할 행동들의 q_value를 반환, x는 board에 대응됨.\n",
        "    return self.model.predict(x)\n",
        "\n",
        "  def getAction(self, state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "       # 무작위 행동 반환\n",
        "      return random.randrange(self.action_size)\n",
        "    else:\n",
        "       # 모델로부터 행동 산출\n",
        "      state = np.float32(state)\n",
        "      q_values = self.model.predict(state)\n",
        "      return np.argmax(q_values[0])\n",
        "\n",
        "  def train(self):\n",
        "    if len(self.replay_memory)<self.min_replay_memory_size: # 충분히 모이지 않으면 학습하지 않는다.\n",
        "      return\n",
        "    \n",
        "    samples = random.sample(self.replay_memory, self.batch_size)\n",
        "    current_input = np.stack([sample[0] for sample in samples]) # current_state들의 array\n",
        "    current_q_values = self.model.predict(current_input)\n",
        "    next_input = np.stack([sample[3] for sample in samples])\n",
        "    next_q_values = self.target_model.predict(next_input)\n",
        "\n",
        "    for i, (current_state, action, reward, _, done) in enumerate(samples):\n",
        "      if done:\n",
        "        next_q_value = reward\n",
        "      else:\n",
        "        next_q_value = reward + self.gamma * np.max(next_q_values[i])\n",
        "      current_q_values[i, action] = next_q_value\n",
        "\n",
        "    hist = self.model.fit(current_input, current_q_values, batch_size=self.batch_size, verbose=0, shuffle=False)\n",
        "    loss = hist.history['loss'][0]\n",
        "    return loss\n",
        "\n",
        "  def increase_target_update_counter(self): # target_model에 model을 업데이트한다. 그걸 세는 함수.\n",
        "    self.target_update_counter += 1\n",
        "    if self.target_update_counter >= self.target_update_freq:\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "      self.target_update_counter = 0\n",
        "\n",
        "  def save(self, model_filepath, target_model_filepath):\n",
        "    self.model.save(model_filepath)\n",
        "    self.target_model.save(target_model_filepath)\n",
        "\n",
        "  def load(self, model_filepath, target_model_filepath):\n",
        "    self.model = keras.models.load_model(model_filepath)\n",
        "    self.target_model = keras.models.load_model(target_model_filepath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HZbfESVy3rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ ==\" __main__\":\n",
        "  agent = Player()\n",
        "  agent.build_model()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsmQPdMNy522",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}